{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90af5e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 26 14:13:31 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |                  Off |\r\n",
      "|  0%   41C    P8              17W / 490W |   4710MiB / 24564MiB |      5%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      1874      G   /usr/lib/xorg/Xorg                          266MiB |\r\n",
      "|    0   N/A  N/A      2038      G   /usr/bin/gnome-shell                        126MiB |\r\n",
      "|    0   N/A  N/A      4704      G   ...ures=SpareRendererForSitePerProcess       24MiB |\r\n",
      "|    0   N/A  N/A      5078      G   ...seed-version=20240322-165906.502000       82MiB |\r\n",
      "|    0   N/A  N/A      6572      C   /home/fish/anaconda3/bin/python            4188MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a88b2d",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880150b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate \n",
    "import transformers\n",
    "import json\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "REPO_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "cache_dir = \"./models\"\n",
    "os.environ['HF_HOME'] = './cache/'\n",
    "environment = \"local\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34606588",
   "metadata": {},
   "source": [
    "## Generating the device map on laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64d5c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/fish/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Show that when we do this neither GPU nor CPU memory increases\n",
    "login(\"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\", add_to_git_credential=True)\n",
    "config = transformers.AutoConfig.from_pretrained(REPO_ID)\n",
    "with accelerate.init_empty_weights():\n",
    "    fake_model = transformers.AutoModelForCausalLM.from_config(config)\n",
    "    \n",
    "device_map = accelerate.infer_auto_device_map(fake_model, max_memory={0: \"10GiB\", \"cpu\": \"6GiB\"})\n",
    "#print(json.dumps(device_map, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab2978",
   "metadata": {},
   "source": [
    "## Loading the model memory efficiently\n",
    "\n",
    "* LLM.int8() quantization\n",
    "* Offloading: Uses GPU memory to the maximum, then CPU and finally memory-mapped chunks on disk\n",
    "  * How offloading works: https://huggingface.co/docs/accelerate/usage_guides/big_modeling\n",
    "\n",
    "**Note**: If you use WSL/Windows, you might run into an issue where the `bitsandbytes` library cannot find the file `libbitsandbytes_cpu.so`. If so follow the instructions [here](https://github.com/TimDettmers/bitsandbytes/issues/156#issuecomment-1474056975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f1560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d592a756c746a8ac7aaba9e2b34031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(REPO_ID)\n",
    "\n",
    "# Check what happens when device_map = auto\n",
    "# This will fail as the model in FP32 precision cannot be fit on CPU\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "# This will fail as the model in FP32 precision cannot be fit on GPU\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\").to(\"cuda\")\n",
    "# This will also fail as the model cannot be fit on GPU fully even with the quantization\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\", device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "if environment == \"local\":\n",
    "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        REPO_ID, \n",
    "        device_map=device_map, \n",
    "        offload_folder=\"/tmp/.offload\",\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True,\n",
    "    )\n",
    "elif environment == \"colab\":\n",
    "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        REPO_ID, \n",
    "        device_map=\"auto\", \n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Environment can only be local/colab. Got {environment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0a2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint in bytes: 13094109184\n"
     ]
    }
   ],
   "source": [
    "# without quantization memory footprint-> 27020779520 (~25.1GB)\n",
    "# The more weights on GPU the better the memory reduction\n",
    "# with quantization memory footprint -> 10710692352 (~10GB)\n",
    "print(f\"Memory footprint in bytes: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c562b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it provided the full list of layers without device map \n",
    "#print(json.dumps(model.hf_device_map, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81c453",
   "metadata": {},
   "source": [
    "## Inferring with the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64b56c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Optymize Protocol is a first-of-its-kind multi-blockchain solution that combines both yield enhancement and risk mitigation for crypto assets. Explain more on the keywords here:\n",
      "\n",
      "Yield enhancement: This refers to the process of increasing the return on investment (ROI) for\n",
      "It took 26.480837741999494s to generate the sequence of 25 tokens (0.9440788937107214 tokens/s).\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# Remember Llama is not instruction finetuned\n",
    "batch = tokenizer(\n",
    "    \"The Optymize Protocol is a first-of-its-kind multi-blockchain solution that combines both yield enhancement and risk mitigation for crypto assets. Explain more on the keywords here\",\n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "# /home/thushv89/anaconda3/envs/ml.torch/lib/python3.9/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
    "batch = {k: v for k, v in batch.items()}\n",
    "n_input_tokens = batch[\"input_ids\"].shape[-1]\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "generated = model.generate(batch[\"input_ids\"].to(\"cuda\"), max_length=n_input_tokens+25)\n",
    "t2 = time.perf_counter()\n",
    "print(tokenizer.decode(generated[0]))\n",
    "n_generated = generated.shape[-1]-batch[\"input_ids\"].shape[-1]\n",
    "print(f\"It took {t2-t1}s to generate the sequence of {n_generated} tokens ({n_generated/(t2-t1)} tokens/s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01092771",
   "metadata": {},
   "source": [
    "# Optymize Special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df313721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050016600ad1448fa7a431476a9f3d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 635, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 868, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 606, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1257, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 533, which is longer than the specified 500\n",
      "/home/fish/anaconda3/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- QUESTION:  what is Optymize?\n",
      "\n",
      "--- QUESTION:  how can i deposite coin on Optymize?\n",
      "\n",
      "--- QUESTION:  what is Optymize's twitter?\n",
      "\n",
      "--- QUESTION:  what is gOPZ tokens?\n",
      "\n",
      "--- QUESTION:  what is Optymize tokenomics?\n",
      "\n",
      "--- QUESTION:  what is Optymize details tokenomics?\n",
      "\n",
      "--- QUESTION:  Optymize Vault Model – How does it works?, detail explaination\n",
      "\n",
      "--- QUESTION:  Optymize Vault Model – How does it works?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def make_rag_chain(model, retriever, rag_prompt = None):\n",
    "    # We will use a prompt template from langchain hub.\n",
    "    if not rag_prompt:\n",
    "        rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # And we will use the LangChain RunnablePassthrough to add some custom processing into our chain.\n",
    "    rag_chain = (\n",
    "            {\n",
    "                \"context\": RunnableLambda(get_question) | retriever | format_docs,\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | rag_prompt\n",
    "            | model\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "def get_question(input):\n",
    "    if not input:\n",
    "        return None\n",
    "    elif isinstance(input,str):\n",
    "        return input\n",
    "    elif isinstance(input,dict) and 'question' in input:\n",
    "        return input['question']\n",
    "    elif isinstance(input,BaseMessage):\n",
    "        return input.content\n",
    "    else:\n",
    "        raise Exception(\"string or dict with 'question' key expected as RAG chain input.\")\n",
    "        \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_dir, device_map=device_map)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "pipe = pipeline(\n",
    "     \"text-generation\", \n",
    "     model=model, \n",
    "     tokenizer=tokenizer,\n",
    "     return_tensors='pt',\n",
    "     max_length=512,\n",
    "     max_new_tokens=512,\n",
    "     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    )\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    " pipeline=pipe,\n",
    " model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")\n",
    "\n",
    "df = pd.read_excel(\"bot2/optymize.xlsx\")\n",
    "data_list = df.values.ravel().tolist()\n",
    "document_list = []\n",
    "\n",
    "for content in data_list:\n",
    "    document = Document(content=content, page_content=content)\n",
    "    document_list.append(document)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(document_list)\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 20})\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "rag_chain = make_rag_chain(llm, retriever, rag_prompt = prompt)\n",
    "\n",
    "questions = [\n",
    "        \"what is Optymize?\",\n",
    "        \"how can i deposite coin on Optymize?\",\n",
    "        \"what is Optymize's twitter?\",\n",
    "        \"what is gOPZ tokens?\",\n",
    "        \"what is Optymize tokenomics?\",\n",
    "        \"what is Optymize details tokenomics?\",\n",
    "        \"Optymize Vault Model – How does it works?, detail explaination\",\n",
    "        \"Optymize Vault Model – How does it works?\"\n",
    "        ]\n",
    "for q in questions:\n",
    "    print(\"\\n--- QUESTION: \", q)\n",
    "    #print(\"* Ans:\\n\", rag_chain.invoke(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404dda27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "llm.invoke(\"what is Optymize?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
