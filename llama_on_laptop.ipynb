{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90af5e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 25 10:41:20 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |                  Off |\r\n",
      "|  0%   45C    P8              27W / 490W |  24130MiB / 24564MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      1970      G   /usr/lib/xorg/Xorg                          201MiB |\r\n",
      "|    0   N/A  N/A      2128      G   /usr/bin/gnome-shell                         51MiB |\r\n",
      "|    0   N/A  N/A      4428      G   ...seed-version=20240322-165906.502000      126MiB |\r\n",
      "|    0   N/A  N/A     41798      G   ...ures=SpareRendererForSitePerProcess       24MiB |\r\n",
      "|    0   N/A  N/A     43684      C   /home/fish/anaconda3/bin/python           23702MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a88b2d",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880150b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to ./cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import accelerate \n",
    "import transformers\n",
    "import json\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "REPO_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "cache_dir = \"./models\"\n",
    "os.environ['HF_HOME'] = './cache/'\n",
    "# Make sure you set this variable according to the environment\n",
    "#environment = \"local\"\n",
    "\n",
    "!huggingface-cli login --token \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34606588",
   "metadata": {},
   "source": [
    "## Generating the device map on laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a64d5c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/fish/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ad1a61ee844b12a0b55cdec08d029f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model.embed_tokens\": 0,\n",
      "    \"model.layers.0\": 0,\n",
      "    \"model.layers.1\": 0,\n",
      "    \"model.layers.2.self_attn\": 0,\n",
      "    \"model.layers.2.mlp.gate_proj\": 0,\n",
      "    \"model.layers.2.mlp.up_proj\": \"cpu\",\n",
      "    \"model.layers.2.mlp.down_proj\": \"cpu\",\n",
      "    \"model.layers.2.mlp.act_fn\": \"cpu\",\n",
      "    \"model.layers.2.input_layernorm\": \"cpu\",\n",
      "    \"model.layers.2.post_attention_layernorm\": \"cpu\",\n",
      "    \"model.layers.3\": \"cpu\",\n",
      "    \"model.layers.4\": \"cpu\",\n",
      "    \"model.layers.5\": \"cpu\",\n",
      "    \"model.layers.6\": \"cpu\",\n",
      "    \"model.layers.7\": \"cpu\",\n",
      "    \"model.layers.8\": \"cpu\",\n",
      "    \"model.layers.9.self_attn\": \"cpu\",\n",
      "    \"model.layers.9.mlp.gate_proj\": \"cpu\",\n",
      "    \"model.layers.9.mlp.up_proj\": \"cpu\",\n",
      "    \"model.layers.9.mlp.down_proj\": \"disk\",\n",
      "    \"model.layers.9.mlp.act_fn\": \"disk\",\n",
      "    \"model.layers.9.input_layernorm\": \"disk\",\n",
      "    \"model.layers.9.post_attention_layernorm\": \"disk\",\n",
      "    \"model.layers.10\": \"disk\",\n",
      "    \"model.layers.11\": \"disk\",\n",
      "    \"model.layers.12\": \"disk\",\n",
      "    \"model.layers.13\": \"disk\",\n",
      "    \"model.layers.14\": \"disk\",\n",
      "    \"model.layers.15\": \"disk\",\n",
      "    \"model.layers.16\": \"disk\",\n",
      "    \"model.layers.17\": \"disk\",\n",
      "    \"model.layers.18\": \"disk\",\n",
      "    \"model.layers.19\": \"disk\",\n",
      "    \"model.layers.20\": \"disk\",\n",
      "    \"model.layers.21\": \"disk\",\n",
      "    \"model.layers.22\": \"disk\",\n",
      "    \"model.layers.23\": \"disk\",\n",
      "    \"model.layers.24\": \"disk\",\n",
      "    \"model.layers.25\": \"disk\",\n",
      "    \"model.layers.26\": \"disk\",\n",
      "    \"model.layers.27\": \"disk\",\n",
      "    \"model.layers.28\": \"disk\",\n",
      "    \"model.layers.29\": \"disk\",\n",
      "    \"model.layers.30\": \"disk\",\n",
      "    \"model.layers.31\": \"disk\",\n",
      "    \"model.norm\": \"disk\",\n",
      "    \"lm_head\": \"disk\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Show that when we do this neither GPU nor CPU memory increases\n",
    "login(\"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\")\n",
    "config = transformers.AutoConfig.from_pretrained(REPO_ID)\n",
    "with accelerate.init_empty_weights():\n",
    "    fake_model = transformers.AutoModelForCausalLM.from_config(config)\n",
    "    \n",
    "device_map = accelerate.infer_auto_device_map(fake_model, max_memory={0: \"3GiB\", \"cpu\": \"6GiB\"})\n",
    "print(json.dumps(device_map, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa57509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This device map was generated using accelerator.infer_auto_device_map() function\n",
    "device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "     'model.layers.0': 0,\n",
    "     'model.layers.1': 0,\n",
    "     'model.layers.2': 0,\n",
    "     'model.layers.3': 0,\n",
    "     'model.layers.4': 0,\n",
    "     'model.layers.5': 0,\n",
    "     'model.layers.6': 0,\n",
    "     'model.layers.7': 0,\n",
    "     'model.layers.8': 0,\n",
    "     'model.layers.9': 0,\n",
    "     'model.layers.10': 0,\n",
    "     'model.layers.11': 0,\n",
    "     'model.layers.12': 0,\n",
    "     'model.layers.13': 0,\n",
    "     'model.layers.14': 'cpu',\n",
    "     'model.layers.15': 'cpu',\n",
    "     'model.layers.16': 'cpu',\n",
    "     'model.layers.17': 'cpu',\n",
    "     'model.layers.18': 'cpu',\n",
    "     'model.layers.19': 'cpu',\n",
    "     'model.layers.20': 'cpu',\n",
    "     'model.layers.21': 'cpu',\n",
    "     'model.layers.22': 'cpu',\n",
    "     'model.layers.23': 'cpu',\n",
    "     'model.layers.24': 'cpu',\n",
    "     'model.layers.25': 'cpu',\n",
    "     'model.layers.26': 'cpu',\n",
    "     'model.layers.27': 'cpu',\n",
    "     'model.layers.28': 'disk',\n",
    "     'model.layers.29': 'disk',\n",
    "     'model.layers.30': 'disk',\n",
    "     'model.layers.31': 'disk',\n",
    "     'model.norm': 'disk',\n",
    "     'lm_head': 'disk'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab2978",
   "metadata": {},
   "source": [
    "## Loading the model memory efficiently\n",
    "\n",
    "* LLM.int8() quantization\n",
    "* Offloading: Uses GPU memory to the maximum, then CPU and finally memory-mapped chunks on disk\n",
    "  * How offloading works: https://huggingface.co/docs/accelerate/usage_guides/big_modeling\n",
    "\n",
    "**Note**: If you use WSL/Windows, you might run into an issue where the `bitsandbytes` library cannot find the file `libbitsandbytes_cpu.so`. If so follow the instructions [here](https://github.com/TimDettmers/bitsandbytes/issues/156#issuecomment-1474056975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92f1560b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mLlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(REPO_ID)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Check what happens when device_map = auto\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# This will fail as the model in FP32 precision cannot be fit on CPU\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# This will also fail as the model cannot be fit on GPU fully even with the quantization\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\", device_map=\"auto\", load_in_8bit=True)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m environment \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1330\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1330\u001b[0m requires_backends(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_backends)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1318\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1316\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(REPO_ID)\n",
    "\n",
    "# Check what happens when device_map = auto\n",
    "# This will fail as the model in FP32 precision cannot be fit on CPU\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "# This will fail as the model in FP32 precision cannot be fit on GPU\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\").to(\"cuda\")\n",
    "# This will also fail as the model cannot be fit on GPU fully even with the quantization\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\", device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "if environment == \"local\":\n",
    "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        REPO_ID, \n",
    "        device_map=device_map, \n",
    "        offload_folder=\"/tmp/.offload\",\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True,\n",
    "    )\n",
    "elif environment == \"colab\":\n",
    "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        REPO_ID, \n",
    "        device_map=\"auto\", \n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Environment can only be local/colab. Got {environment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0a2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint in bytes: 10710692352\n"
     ]
    }
   ],
   "source": [
    "# without quantization memory footprint-> 27020779520 (~25.1GB)\n",
    "# The more weights on GPU the better the memory reduction\n",
    "# with quantization memory footprint -> 10710692352 (~10GB)\n",
    "print(f\"Memory footprint in bytes: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c562b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model.embed_tokens\": 0,\n",
      "    \"model.layers.0\": 0,\n",
      "    \"model.layers.1\": 0,\n",
      "    \"model.layers.2\": 0,\n",
      "    \"model.layers.3\": 0,\n",
      "    \"model.layers.4\": 0,\n",
      "    \"model.layers.5\": 0,\n",
      "    \"model.layers.6\": 0,\n",
      "    \"model.layers.7\": 0,\n",
      "    \"model.layers.8\": 0,\n",
      "    \"model.layers.9\": 0,\n",
      "    \"model.layers.10\": 0,\n",
      "    \"model.layers.11\": 0,\n",
      "    \"model.layers.12\": 0,\n",
      "    \"model.layers.13\": 0,\n",
      "    \"model.layers.14\": \"cpu\",\n",
      "    \"model.layers.15\": \"cpu\",\n",
      "    \"model.layers.16\": \"cpu\",\n",
      "    \"model.layers.17\": \"cpu\",\n",
      "    \"model.layers.18\": \"cpu\",\n",
      "    \"model.layers.19\": \"cpu\",\n",
      "    \"model.layers.20\": \"cpu\",\n",
      "    \"model.layers.21\": \"cpu\",\n",
      "    \"model.layers.22\": \"cpu\",\n",
      "    \"model.layers.23\": \"cpu\",\n",
      "    \"model.layers.24\": \"cpu\",\n",
      "    \"model.layers.25\": \"cpu\",\n",
      "    \"model.layers.26\": \"cpu\",\n",
      "    \"model.layers.27\": \"cpu\",\n",
      "    \"model.layers.28\": \"disk\",\n",
      "    \"model.layers.29\": \"disk\",\n",
      "    \"model.layers.30\": \"disk\",\n",
      "    \"model.layers.31\": \"disk\",\n",
      "    \"model.norm\": \"disk\",\n",
      "    \"lm_head\": \"disk\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check if it provided the full list of layers without device map \n",
    "print(json.dumps(model.hf_device_map, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81c453",
   "metadata": {},
   "source": [
    "## Inferring with the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b64b56c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would explain a blackhole to a 5 year old as a place where the laws of physics don't apply.\n",
      "I'm not sure I'd go that far.\n",
      "It took 69.4328759409982s to generate the sequence of 25 tokens (0.36005998111390614 tokens/s).\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# Remember Llama is not instruction finetuned\n",
    "batch = tokenizer(\n",
    "    \"I would explain a blackhole to a 5 year old as\",\n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "# /home/thushv89/anaconda3/envs/ml.torch/lib/python3.9/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
    "batch = {k: v for k, v in batch.items()}\n",
    "n_input_tokens = batch[\"input_ids\"].shape[-1]\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "generated = model.generate(batch[\"input_ids\"].to(\"cuda\"), max_length=n_input_tokens+25)\n",
    "t2 = time.perf_counter()\n",
    "print(tokenizer.decode(generated[0]))\n",
    "n_generated = generated.shape[-1]-batch[\"input_ids\"].shape[-1]\n",
    "print(f\"It took {t2-t1}s to generate the sequence of {n_generated} tokens ({n_generated/(t2-t1)} tokens/s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c998b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
